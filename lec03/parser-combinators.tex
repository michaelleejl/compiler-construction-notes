\chapter{Recursive Descent and Parser Combinators}\label{chapter:parser-combinators}

\begin{center}
    \import{figures}{context}
\end{center}

\section{Overview}
In the previous chapter, we searched for a universal way to describe parser specifications and parsers, concluding that parser specifications are typically \textit{non-ambiguous, deterministic context-free grammars}, and parsers are \textit{deterministic push-down automata}. We also noted that as the result of practical considerations, the space of parser specifications and parser generators is a lot more fragmented and complicated than this universal view, promising to examine this with more specificity. 

In this chapter, we will look at the first such concrete example: the recursive descent parser. 

\begin{center}
{
\sffamily
\def\arraystretch{1.5}%
\newcommand{\pro}[0]{\textcolor{selected}{$+$}}
\newcommand{\con}[0]{\textcolor{highlight}{$-$}}
    \begin{tabular}[t]{|l|l|}
    \hline
     \multicolumn{2}{|l|}{\Large \textbf{Fact Sheet}}  \\ \hline
     \textbf{Type} & Top-Down, Left-Most Derivation \\ \hline
     \textbf{Specification} & Non-left-recursive, unambiguous CFGs \\ \hline
     \textbf{Properties} & 
        \begin{tabular}[t]{@{}ll@{}}  
        \pro & Easy to construct \\ 
        \con & Inefficient $^*$ \\ 
        \con & Not expressive \\ 
        \con & No clean declarative reading $^*$ 
        \end{tabular} \\ \hline
\end{tabular}
}
\end{center}

\section{Recursive Descent Parser}
The best way to get a feeling for the recursive descent parser is to actually see an example. Consider the grammar $G_3$, which produces the language of arithmetic expressions. It has the following production rules
\[
\begin{array}{lcl}
  E  &\to& T \;E' \\
  E' &\to& + \;T\; E' \;\mid\; \epsilon\\
  T  &\to& F\; T'\\
  T' &\to& * \;F\; T' \;\mid\; \epsilon\\
  F  &\to& (\;E\;) \;\mid\; \text{id}\\
\end{array}
\]
Note that $L(G_3) = L(G_2) = L(G_1)$, where $G_2$ is defined on \Cpageref{section:ambiguity-elim} and $G_1$ on \Cpageref{section:cfg-defn}.

A recursive descent parser for $G_3$ looks as follows

\begin{code}[A recursive descent parser for $G_3$ in \texttt{OCaml}]
\label{code:ocaml-recursive-descent}
\begin{minted}[bgcolor=backcolour, linenos, texcomments]{ocaml}
let rec
    e toks = e' (t toks)                      (* $E \rightarrow T E'$ *)
and e' = function
  | ADD :: toks -> e' (t toks)                (* $E' \rightarrow + T E'$ *)
  | toks        -> toks                       (* $E' \rightarrow \epsilon$ *)
and t toks = t' (f toks)                      (* $T  \rightarrow F T'$ *)
and t' = function
  | MUL :: toks -> t' (f toks)                (* $T'  \rightarrow * F T'$ *)
  | toks        -> toks                       (* $T' \rightarrow \epsilon$ *)
and f = function
    | LPAREN :: toks ->
      (match e toks with
      | RPAREN :: toks -> toks                (* $F \rightarrow (E)$ *) 
      | _              -> failwith "RPAREN")
    | IDENT _ :: toks -> toks                 (* $F \rightarrow \text{id}$ *)
    | _               -> failwith "F"
\end{minted}
\end{code}

We hope you notice that the structure of the recursive descent parser almost immediately mirrors the structure of our context-free grammar. We will now make this correspondence more explicit.

Assume that our parser is parsing the word $(x + y) * z$ --- that is, verifying it has the right structure and constructing a parse tree. The starting symbol is $E$, so in other words we are verifying that $E$ can expand to $(x + y) * z$

Assume there exists a derivation sequence
\[E \Rightarrow^{+} (x+y) * z\]
There is only one production for $E$, so the derivation sequence must be of the form
\[
\begin{array}{rll}
E&\Rightarrow&TE'\\
&{\Rightarrow^{+}}&(x+y) * z
\end{array}
\]
This means the derivation sequence $E \Rightarrow^{+} (x+y) * z$ exists if and only if the derivation sequence $TE' \Rightarrow^{+} (x+y) * z$.

Put another way, we must be able to find $u$ and $v$ such that $(x + y) * z$ can be split into $uv$, and there exists derivation sequences
\[
\begin{array}{rcl}
T&{\Rightarrow^{+}}&u\\
{E'}&{\Rightarrow^{+}}&v
\end{array}
\]
Operationally, assume we had a function \texttt{t} that could take $uv$, match $u$, and return $v$. Assume we had a function \texttt{e'} that could take $v$ and return $\epsilon$ (indicating success). Then we could construct a function \texttt{e} that \textit{chains} \texttt{t} and \texttt{e'}, as follows
\begin{minted}[bgcolor=backcolour, texcomments]{ocaml}
let e uv =
    let v = t uv in
    e' v
\end{minted}
We can inline \texttt{v} as such
\begin{minted}[bgcolor=backcolour, texcomments]{ocaml}
let e uv = e' (t uv)
\end{minted}
Renaming \texttt{uv} to \texttt{toks}, we get the definition of \texttt{e} as in \Cref{code:ocaml-recursive-descent}.

Therefore, we can say each function takes a sequence of \texttt{toks}, matches some prefix of \texttt{toks}, and returns some suffix of \texttt{toks}.

Note how the language features --- higher-order functions and pattern matching --- are effectively deployed to build a parser. This is what we mean by recursive descent parsers being ``easy to construct''.

Having seen a concrete example, we can now define what a recursive parser \textit{is}

\begin{definition}[Recursive Descent Parser]
    A recursive descent parser is a top-down, left-most parser that uses a set of mutually recursive functions, where each function parses a non-terminal in the grammar.
\end{definition}

\subsection{Backtracking and Predictive Parsers}
We should also note that this is not the only way, or even a canonical way, to write a recursive descent parser. You'll often see recursive descent parsers written with a lot of backtracking \texttt{try / with} statements.

For example, given the rule 
\[
\begin{array}{rcl}
     E'&\to&+TE'  \\
     &\mid&\epsilon
\end{array}
\]
A naÃ®ve recursive descent parser might write 
\begin{minted}[bgcolor=backcolour, texcomments]{ocaml}
...
and e ' toks = 
    try e ' ( t toks )
    with Fail _ -> toks
...
\end{minted}
Which explores the first branch, and, if it fails, tries the second. Such a recursive descent parser only has one job -- matching. However, our recursive descent parser is smarter, using the \textit{next} token to decide, \textit{deterministically} what to do. We choose the first rule if, and only if, the next token is \texttt{PLUS}, choosing the second otherwise. 
\begin{minted}[bgcolor=backcolour, texcomments]{ocaml}
...
and e ' = function 
    | PLUS::toks -> e' (t toks)
    | toks -> toks
...
\end{minted}
This means our recursive descent parser has \textit{two} jobs, \textit{prediction} (predicting which branch to take) and \textit{matching}. In addition, we can say it uses \textit{the next token} in the token sequence to make predictions, and in doing so, it is able to reduce non-determinism.

Hence, we call our parser a \textbf{predictive parser}.

\begin{definition}[Predictive Parser]
    A predictive parser is a parser that does not need to backtrack.
\end{definition}

A predictive parser is more efficient, taking time linear in the number of tokens. Parsers written in this backtracking style can be extremely inefficient, taking time \textit{exponential} in the number of tokens. However, one advantage is that they can parse non-deterministic (but unambiguous) context-free grammars: if you don't know which branch to choose, use backtracking to try both!

At this point, we have \textbf{told} you that our parser does not need to backtrack. That is, we have \textit{asserted} that any ambiguity can be resolved using a single token of look-ahead. But we have not \textit{justified} ourselves, and you should be skeptical. Justification will come in \Cref{chapter:llk}.

\section{Left Recursive Grammars}
Inefficiency, however, is \textit{not} the biggest problem with recursive descent parsers. That dishonour belongs to the left-recursive problem. Consider $G_3$ --- why do you think it looks so weird? Why did we not use $G_2$?

Consider the $G_2$ production rule
\[ E \rightarrow E + T \]
We call this rule \textit{left-recursive}: \textit{recursive} because $E$ appears both on the left and right-hand side of the arrow, and \textit{left} because it is the first symbol on the right hand side of the token.

Perhaps you can spot why this won't work! Turning this into a recursive descent function, we get 
\begin{minted}[bgcolor=backcolour]{ocaml}
    let rec e toks = match e toks with (*infinite loop*)
      | PLUS :: toks -> t toks
\end{minted}

The \textit{first} thing \texttt{e} does is call itself. 

Note that left-recursion is even more sinister than this. The following grammar is also left recursive
\[\begin{array}{lcl}
     A & \rightarrow & Cd  \\
     B & \rightarrow & Ce \\
     C & \rightarrow & A \mid B \mid f
\end{array}\]
Because we can go from $A \rightarrow Cd \rightarrow Ad$. Importantly, in this sequence of expansions, there are no terminals on the left. If there \textit{was} a terminal, this would be fine, since you'd be doing some work. However, as it stands, you would be looping in a non-productive, futile way.

By the way, we call the first type of left-recursion (where there is a rule of the form $A \rightarrow A\alpha$) \textit{direct} left-recursion, and the second type \textit{indirect} left-recursion.

Therefore, formally, a grammar is left-recursive if there exists a non-terminal $A$ and sequence $\alpha$ such that
\[ A \Rightarrow^{+} A\alpha \]
\subsection{Eliminating Left Recursion}
Thankfully, you can \textit{always} eliminate left recursion. We can do this in a two-step process.

\begin{enumerate}
    \item Convert indirect left recursion to direct left recursion.
    \item Eliminate direct left recursion.
\end{enumerate}

We can convert indirect left recursion to direct left recursion via a process of inlining. Basically, establish an order on non-terminals (in this case, $A \prec B \prec C$). Starting with the greatest, whenever you see a non-terminal, replace it with its productions, as such:
\[\begin{array}{lcl}
     A & \rightarrow & Cd  \\
     B & \rightarrow & Ce \\
     C & \rightarrow & Cd \mid Ce \mid f
\end{array}\]
Which in just a single iteration, eliminates indirect left recursion.

To eliminate direct recursion, note that whenever you have a left-recursive rule of the form
\[ A \rightarrow A\alpha \]
You must also have a left-recursive base-case, of the form
\[A \rightarrow \beta \]\footnote{where $\beta$ does not begin with $A$}
For example, considering $G_2$,
\[
\begin{array}{rcl}
E &\to& E + T \mid T\\
T &\to& T * F \mid F\\
F &\to& (E) \mid id\\
\end{array}
\]
Whenever we have a direct left-recursive rule $E \to E + T$, we also have a base case $E \to T$.

The fundamental idea is that when we have a left-recursive rule $E \to E + T$, we do want the left-recursive $E$ to expand to $T$ at some point (we want to hit the base case), we're just not sure which point that is. Therefore, conceptually, we can replace $E$ with $T E'$
\[ E \to T E' \]
We now need to figure out what $E'$ can expand to. To do this, let's consider all the things $E$ can expand to
\[
\begin{array}{rcl}
   E&\Rightarrow^{+}&T\\
   E&\Rightarrow^{+}&T+T\\
   E&\Rightarrow^{+}&T+T+T\\
   \ldots&
\end{array}
\]
We replace $E$ with $TE'$
\[
\begin{array}{rcl}
   TE'&\Rightarrow^{+}&T\\
   TE'&\Rightarrow^{+}&T+T\\
   TE'&\Rightarrow^{+}&T+T+T\\
   \ldots&
\end{array}
\]
Dropping $T$ from both sides
\[
\begin{array}{rcl}
   E'&\Rightarrow^{+}&\epsilon\\
   E'&\Rightarrow^{+}&+T\\
   E'&\Rightarrow^{+}&+T+T\\
   \ldots&
\end{array}
\]
Hopefully, you can see that we can describe this using the rule
\[ E' \rightarrow +TE' \mid \epsilon \]
We have thus eliminated left-recursion. Note also that this is exactly the rule for $E'$ in $G_2$.

More generally, whenever we have a direct left recursive production rule of the form
\[ A \rightarrow A\alpha_1 \mid A\alpha_2 \mid \ldots \mid \beta_1 \mid \beta_2 \mid \ldots \]
We can replace this with the following pair of rules
\[
\begin{array}{rcl}
   A&\rightarrow&\beta_1A' \mid \beta_2A' \ldots \\
   A'&\rightarrow&\epsilon \mid \alpha_1A' \mid \alpha_2A' \ldots \\
\end{array}
\]
In order to solve the left-recursive problem, our grammar no longer has a clean, declarative reading.

\section{Recovering the Push-Down Automaton}
One final point worth noting is that the recursive-descent parser we have seen looks nothing like the push-down automata we claimed were at the heart of parsers. For example, where has the stack gone? What are states?

The state you can think of as the instruction that you are currently executing.

The stack is \textit{implicit} in the \textit{call stack} of the program. You may recall from \textsf{Part IA Foundations of Computer Science} that programming languages have call stacks, that store ``the rest of the work that needs to be done after the function executes''. Let's examine the call stack when we parse $x + y * z$

\begin{minipage}[t]{0.5\textwidth}
    \begin{tabular}{rl}
    \ttfamily
    & {\texttt{e toks}}\\[1ex]
    $\leadsto$ & {\texttt{e' (t toks)}}\\[1ex]
    $\leadsto$ & {\texttt{e' (t' f toks)}}\\[1ex]
    $\leadsto$ & $\ldots$
    \end{tabular}
\end{minipage}%
\begin{minipage}[t]{0.5\textwidth}
    \begin{tabular}{ll}
    & {\text{Stack: $E$}}\\[1ex]
    $\text{Pop $E$, Push $E'$, Push $T$}$ & {\text{Stack: $T, E'$}}\\[1ex]
    $\text{Pop $T$, Push $T'$, Push $F$}$ & {\text{Stack: $F, T', E'$}}\\[1ex]
    $\ldots$ & $\ldots$
    \end{tabular}
\end{minipage}%
\section{Parser Combinators$^*$}\label{section:parser-combinator}
\textit{This optional section is based on \href{https://www.cl.cam.ac.uk/~jdy22/papers/a-typed-algebraic-approach-to-parsing.pdf}{A Typed, Algebraic Approach to Parsing} by} \citet{krishnaswami-2019}.

Recursive descent might seem ad-hoc and unsatisfactory. We can make this less ad-hoc by the technique of parser combinators. A parser combinator is a higher-order function that allow us to construct larger parsers from smaller parsers. A parser combinator \textit{library} exposes a suite of combinators that lets us build parsers in a less ad-hoc way. In fact, if you build your library in a smart way, it can negate a lot of the disadvantages of parser combinators! 

We begin by investigating the parser combinator, and then discuss how it mitigates some of the disadvantages of recursive descent. The key observation is to notice that many functions follow the same format, for example

\begin{minted}[bgcolor=backcolour, texcomments]{ocaml}
e toks = e' (t toks)
(*$\ldots$*)
t toks = t' (f toks)
\end{minted}

Let's try to extract out the underlying, general structure, and make it an interface! Recall that the \texttt{e} function takes two parsers, \texttt{e'} and \texttt{t}, and creates a parser that matches \texttt{t} \textit{then} \texttt{e'}.

% Assume we have a parser \texttt{p} that matches some CFG $A$, with production rules $P_A$. Assume we have a parser \texttt{q} that matches some CFG $B$, with production rules $P_B$. We can construct a CFG $C$ that matches patterns of the form ``$A$ then $B$'' 
% \[
% \begin{array}{lcl}
%      C & \rightarrow & AB \\
%      A & \rightarrow &\ldots\\
%        & \rightarrow &(\text{production rules for } A)\\
%      B & \rightarrow &\ldots\\
%        & \rightarrow &(\text{production rules for } B)
% \end{array}
% \]
We can generalise this technique to construct a \textit{parser combinator} that, given \texttt{p} and \texttt{q}, produces a parser that matches ``\texttt{p} then \texttt{q}''

\begin{minted}[bgcolor=backcolour, texcomments]{ocaml}
let seq p q = 
   let pq toks = q p (toks)
\end{minted}
And then we can \textit{use} the parser combinator 
\begin{minted}[bgcolor=backcolour, texcomments]{ocaml}
let e = seq t e'
\end{minted}
As another example, we can consider
\begin{minted}[bgcolor=backcolour, texcomments]{ocaml}
e' toks =
| ADD :: toks -> e' (t toks)
| toks        -> toks
\end{minted}
Which attempts to match \textit{either} \texttt{e'} then \texttt{t} \textit{or} $\epsilon$. We can generalise this technique to construct a parser combinator that, given parsers \texttt{p} and \texttt{q}, produces a parser that matches ``\texttt{p} or \texttt{q}''

\begin{minted}[bgcolor=backcolour, texcomments]{ocaml}
let alt p q = 
   let pq toks = try p toks except _ -> q toks
\end{minted}

Which we can then \textit{use} to create the \texttt{e'} function
\begin{minted}[bgcolor=backcolour, texcomments]{ocaml}
let case1 = function
   | ADD :: toks -> (seq e' t) toks
   | _           -> fail
and case2 = function
   | toks -> toks
and rec e' = alt case1 case2
\end{minted}

\subsection{An Example Parser Combinator Library}
\begin{minted}[bgcolor=backcolour]{ocaml}
type 'a t
val eps : unit t
val bot : 'a t
val chr : char -> char t
val seq : 'a t -> 'b t -> ('a * 'b) t
val alt : 'a t -> 'a t -> 'a t
val fix : ('b t -> 'b t) -> 'b t
val map : ('a -> 'b) -> 'a t -> 'b t
\end{minted}
\texttt{'a t} is the type of a parser that consumes a sequence of tokens and returns a value of type \texttt{'a}. 

You can see, for example, that sequencing two parsers, one of type \texttt{'a t}, and one of type \texttt{'b t}, creates a parser that takes a sequence of tokens and returns a \textit{node}, whose left child is of type \texttt{'a} and right child is of type \texttt{'b}.

\texttt{eps} is a base parser that parses the empty string. \texttt{bot} is the parser that always fails. \texttt{chr} is a function that takes in a character, like \texttt{a}, and returns a parser for the character. 

\texttt{seq} and \texttt{alt} are parser combinators, as described in the previous section.

\texttt{fix} is a fixed-point combinator for parsers. We don't want to get drawn into a digression on fixed points. As you will learn in \textsf{Part IB Computation Theory}, fixed points are the fundamental mechanism behind recursion. In effect, this combinator allows us to define parsers recursively.

\texttt{map} allows us to wrap a user-defined function around the output of a parser.

\subsection{User-Defined Abstractions Over Parsing Primitives}
One advantage of parser combinators is that they allow users to define abstractions over parsing primitives. 

First, some syntactic sugar.
\begin{minted}[bgcolor=backcolour]{ocaml}
let always x = fun _ -> x
let (++) = seq
let (==>) p f = map f p
\end{minted}
We define an \texttt{always} function that will return \texttt{x} no matter its input. We also define infix operators \texttt{++} and \texttt{==>}, such that 
\[
\begin{array}{rcl}
\texttt{seq p q} &\cong& \texttt{p ++ q}\\
\texttt{map f p}&\cong&\texttt{p ==> f}
\end{array}
\]
Which allows us to define user abstractions such as \texttt{any}, a generalisation of \texttt{alt} that works for more than two arguments
\begin{minted}[bgcolor=backcolour]{ocaml}
let any (ps: 'a list) = foldl bot ps
\end{minted}
\texttt{option}, which accepts either the empty string or whatever \texttt{p} can parse
\begin{minted}[bgcolor=backcolour]{ocaml}
let option p = any [eps ==> always None;
                    p   ==> fun x -> Some x
                   ]
\end{minted}
And finally \texttt{star}, which, if you remember the definition of the Kleene star, matches whatever \texttt{p} can parse any number of times
\begin{minted}[bgcolor=backcolour]{ocaml}
let star p = fix (q -> any [eps    ==> always [];
                            p ++ q ==> fun (x, xs) -> x::xs
                           ])
\end{minted}
Whenever you see \texttt{fix} (for \texttt{fix}-ed point), think recursion! So the aforementioned code is equivalent to the perhaps more natural
\begin{minted}[bgcolor=backcolour]{ocaml}
let rec star p = any [eps    ==> always [];
                      p ++ (star p) ==> fun (x, xs) -> x::xs
                     ])
\end{minted}
\subsection{Infix Operators in Parser Combinators}
Equipped with the ability to build abstractions, we can get a more declarative reading of our grammar. Recall that what we would really like to do is declare a grammar that has an infix $+$ operator, something that looks like 
\[ E \rightarrow E + T\]
Or better still, declare that we want a \textit{left-associative} $+$ operator.

These have very clean declarative readings. However, because of the inability of a recursive descent parser to handle left-recursion, we are stuck with the awkward
\[\begin{array}{rcl}
     E & \to & T E'  \\
     E' & \to & + TE' \mid \epsilon 
\end{array}\]
User abstraction allows us to take a parser for an infix-operator, \texttt{op} (for example, \texttt{op} recognises $+$) a parser that recognises a base expression, \texttt{base} (for example, a parser that recognises $E$), and build a parser for the right-associative operator \texttt{base op base} (a parser that recognises $E + E$). 

The idea is that the resulting parser, which we will call \texttt{r}, will be able to parse (unambiguously) words with the structure
\[\begin{array}{l}
     E\\
     E+E\\
     E+(E+E)\\
     \ldots
\end{array}\]
Where the brackets indicate where associativity has been used to handle ambiguity.

We shall now see how to to build such an \texttt{r} using the parser combinator library.

Assume we have a \texttt{base} parser that can parse expressions of the form $E$. Once we have applied this \texttt{base} parser, we want our parser to match the following structures,
\[\begin{array}{l}
     \epsilon\\
     +E\\
     +E+E\\
     +E+(E+E)\\
     \ldots
\end{array}\]
Assume we had some parser \texttt{q} that matches the above patterns. Then, we can construct our desired parser using sequencing. In other words, we have the following equation:
\[\texttt{r} = \texttt{base ++ q}\]
The $\epsilon$ is a little annoying, since it doesn't follow the same structure as the other patterns. We therefore seek to cut it out. We can do this by defining \texttt{q} as matching \textit{either}
$\epsilon$ or:
\[\begin{array}{l}
     +E\\
     +E+E\\
     +E+(E+E)\\
     \ldots
\end{array}\]
Assume we had some parser \texttt{q'} that could parse the above sequence. Then, we have the following equation
\[
\begin{array}{rcl}
     \texttt{q}&=&\texttt{alt eps q'}  \\
     &=& \texttt{option q'} 
\end{array}
\]
Substituting into our original equation, we have
\[\texttt{r} = \texttt{base ++ option q'} \]
Clearly \texttt{q'} can be described as \texttt{op ++ q''}, where \texttt{op} matches $+$ and \texttt{q''} matches patterns of the form:
\[\begin{array}{l}
     E\\
     E+E\\
     E+(E+E)\\
     \ldots
\end{array}\]
So we have
\[\texttt{r} = \texttt{base ++ option (op ++ q'')}\]
But \texttt{q''} matches exactly the same patterns as \texttt{r}! So we have  
\[\texttt{r} = \texttt{base ++ option (op ++ r)}\]
In other words, \texttt{r} is a fixed point for the function
\[\texttt{fun p -> base ++ option (op ++ p)}\]
And so we have 
\[\texttt{r} = \texttt{fix (fun p -> base ++ option (op ++ p))}\]

This is almost exactly how the \texttt{infixr} function is defined.

\begin{minted}[bgcolor=backcolour]{ocaml}
val infixr : ('a -> 'a -> 'a) t -> 'a t -> 'a t
let infixr op base =
    fix (fun p ->
        base ++ option (op ++ p) ==> function
        | (e, None) -> e
        | (e, Some(f, e')) -> f e e')
\end{minted}
The only difference is the \texttt{map}, which just musters the resulting trees into the right shape. Recall that sequencing constructs a \textit{tree} via a \textit{pair}. But if the right branch is \texttt{None}, then we can perform the following transformation, to make the parse tree have a nicer shape
\begin{center}
    \begin{tikzpicture}
           \node(s) at (1.5cm, 0) {\texttt{e}};
           
          \node[] at ($(s.west) - (0.26cm,0.05cm)$) {$\Rightarrow$};
        
          \begin{scope}[xshift=0cm, yshift=-0.14cm]
            \Tree [.$\textsf{Node}$
                     {$\texttt{e}$} 
                     {$\texttt{\textbf{None}}$}
                  ]
          \end{scope};
     \end{tikzpicture}
\end{center}
    
If the right branch is \texttt{Some(+, e')}, then we want to perform the following transformation

\begin{center}
    \begin{tikzpicture}
           \node(s) at (3cm, 0) {}

           \begin{scope}[xshift=3cm, yshift=-0.14cm]
            \Tree [.$\texttt{\textbf{+}}$
                     {$\texttt{e}$} 
                     {$\texttt{e'}$}
                  ]
          \end{scope};
           
          \node[] at ($(s.west) - (1.26cm,0.05cm)$) {$\Rightarrow$}
        
          \begin{scope}[xshift=0cm, yshift=-0.14cm]
            \Tree [.$\textsf{Node}$
                     {$\texttt{e}$} 
                     {$\texttt{\textbf{Some}(+, e')}$}
                  ]
          \end{scope};
     \end{tikzpicture}
\end{center}

We can do something similar to build a left-associative equivalent \texttt{infixl}, though we elide the details.

The end result is that we can get the user to specify an operator and an associativity, and \textit{automatically} generate the desired parser for that operator
\begin{minted}[bgcolor=backcolour]{ocaml}
let infix_op base ao = (*ao stands for associativity + operator*)
    match ao with
    | Left,  op  -> infixl op base
    | Right, op -> infixr op base
\end{minted}
In fact, it is even possible to expose a single function, that allows the user to specify a \textit{list} of operators and their associativities, ordered by precedence, and build an unambiguous parser that meets this specification.
\begin{minted}[bgcolor=backcolour]{ocaml}
let infix aos base = foldl infix_op base aos
\end{minted}
Which can be used as follows:
\begin{minted}[bgcolor=backcolour]{ocaml}
infix [(Left, chr '*' ==> always Float.mul);
       (Left, chr '+' ==> always Float.add)]]
      base 
\end{minted}
Remember (from \textsf{Part IA Foundations of Computer Science}) that a left-fold, \texttt{foldl}, takes
\begin{enumerate}
    \item A function (in this case \texttt{infix\_op})
    \item A base value (in this case, \texttt{base})
    \item A list (in this case, \texttt{aos})
\end{enumerate}

At each element of the list \texttt{ao}, \texttt{foldl} will apply \texttt{infix\_op} to \texttt{base} and \texttt{ao}, and make this the new \texttt{base}. 

Hence, assuming our \texttt{base} parses patterns of the form $E$, then after the left-fold has processed \texttt{(Left, chr '*' ==> always Float.mul)}, the new base will be able to process $E * E$. After the left-fold has processed \texttt{(Left, chr '+' ==> always Float.add)}, the new base will be able to process $(E*E) + (E*E)$, for example. (Note that the order indicates precedence!)


% \subsection{Looking Ahead: Linear-Time Parser Combinators}
% Finally, we can use the \textit{type} system to ensure that our parser combinator has \textit{no backtracking} --- that is, it is guaranteed to take time linear in the number of tokens. The details can be found in the paper, but a very understandable introduction can be found \href{https://semantic-domain.blogspot.com/2023/07/linear-time-parser-combinators.html}{here}.