@article{kelsey-1995,
	author = {Kelsey, Richard A.},
	journal = {SIGPLAN notices},
	month = {3},
	number = {3},
	pages = {13--22},
	title = {{A correspondence between continuation passing style and static single assignment form}},
	volume = {30},
	year = {1995},
	doi = {10.1145/202530.202532},
	url = {https://doi.org/10.1145/202530.202532},
}

@article{yallop-2023,
	author = {Yallop, Jeremy and Xie, Ningning and Krishnaswami, Neel},
	journal = {Proceedings of the ACM on programming languages},
	month = {6},
	number = {PLDI},
	pages = {1194--1217},
	title = {{flap: A Deterministic Parser with Fused Lexing}},
	volume = {7},
	year = {2023},
	doi = {10.1145/3591269},
	url = {https://doi.org/10.1145/3591269},
}

@book{mccarthy-1963,
	author = {McCarthy, John},
	booktitle = {Studies in logic and the foundations of mathematics},
	month = {1},
	pages = {33--70},
	title = {{A basis for a mathematical theory of computation)}},
	year = {1963},
	doi = {10.1016/s0049-237x(08)72018-4},
	url = {https://doi.org/10.1016/s0049-237x(08)72018-4},
}

@article{griffin-1990,
	author = {Griffin, Timothy G.},
	journal = {POPL '90: Proceedings of the 17th ACM SIGPLAN-SIGACT symposium on Principles of programming languages},
	month = {1},
	title = {{A formulae-as-type notion of control}},
	year = {1990},
	doi = {10.1145/96709.96714},
	url = {https://doi.org/10.1145/96709.96714},
}

@misc{drup-2018,
	author = {Drup},
	month = {5},
	title = {{An OCaml quine}},
	year = {2018},
	url = {https://drup.github.io/2018/05/30/quine/},
}

@book{nielsen-1993,
	author = {Nielsen, Jakob},
	month = {1},
	title = {{Usability Engineering}},
	year = {1993},
	url = {https://dl.acm.org/citation.cfm?id=2821575},
}

@article{owens-2009,
	author = {Owens, Scott and Reppy, John and Turon, Aaron},
	journal = {Journal of functional programming},
	month = {3},
	number = {2},
	pages = {173--190},
	title = {{Regular-expression derivatives re-examined}},
	volume = {19},
	year = {2009},
	doi = {10.1017/s0956796808007090},
	url = {https://doi.org/10.1017/s0956796808007090},
}

@book{sebesta-1993,
	author = {Sebesta, Robert W.},
	month = {3},
	title = {{Concepts of programming languages (2nd ed.)}},
	year = {1993},
	url = {https://dl.acm.org/citation.cfm?id=230189},
}

@article{mytkowicz-2014,
	author = {Mytkowicz, Todd and Musuvathi, Madanlal and Schulte, Wolfram},
	journal = {ASPLOS '14: Proceedings of the 19th international conference on Architectural support for programming languages and operating systems},
	month = {2},
	title = {{Data-parallel finite-state machines}},
	year = {2014},
	doi = {10.1145/2541940.2541988},
	url = {https://doi.org/10.1145/2541940.2541988},
}

@article{owens-2009,
	author = {Owens, Scott and Reppy, John and Turon, Aaron},
	journal = {Journal of functional programming},
	month = {3},
	number = {2},
	pages = {173--190},
	title = {{Regular-expression derivatives re-examined}},
	volume = {19},
	year = {2009},
	doi = {10.1017/s0956796808007090},
	url = {https://doi.org/10.1017/s0956796808007090},
}

@book{kernighan-1978,
	author = {Kernighan, Brian W. and Ritchie, Dennis M.},
	month = {1},
	title = {{The C programming language}},
	year = {1978},
	url = {https://usuaris.tinet.cat/bertolin/pdfs/c_programming_language.pdf},
}

@book{sipser-2006,
  added-at = {2014-03-03T20:31:33.000+0100},
  author = {Sipser, Michael},
  biburl = {https://www.bibsonomy.org/bibtex/2317441ce12cacafa82cd32c4b2ed95cc/ytyoun},
  edition = {Second},
  interhash = {6d6f89523b99e25120368e673ca59c01},
  intrahash = {317441ce12cacafa82cd32c4b2ed95cc},
  isbn = {7111173279 9787111173274},
  keywords = {automata complexity computation gadget hamiltonian np-hardness sipser textbook},
  publisher = {Course Technology},
  refid = {660595094},
  timestamp = {2016-12-04T08:23:10.000+0100},
  title = {Introduction to the Theory of Computation},
  year = 2006
}

@inproceedings{krishnaswami-2019,
author = {Krishnaswami, Neelakantan R. and Yallop, Jeremy},
title = {A typed, algebraic approach to parsing},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314625},
doi = {10.1145/3314221.3314625},
abstract = {In this paper, we recall the definition of the context-free expressions (or µ-regular expressions), an algebraic presentation of the context-free languages. Then, we define a core type system for the context-free expressions which gives a compositional criterion for identifying those context-free expressions which can be parsed unambiguously by predictive algorithms in the style of recursive descent or LL(1). Next, we show how these typed grammar expressions can be used to derive a parser combinator library which both guarantees linear-time parsing with no backtracking and single-token lookahead, and which respects the natural denotational semantics of context-free expressions. Finally, we show how to exploit the type information to write a staged version of this library, which produces dramatic increases in performance, even outperforming code generated by the standard parser generator tool ocamlyacc.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {379–393},
numpages = {15},
keywords = {Kleene algebra, context-free languages, parsing, type theory},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{sperber-1995,
author = {Sperber, Michael and Thiemann, Peter},
title = {The essence of LR parsing},
year = {1995},
isbn = {0897917200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/215465.215579},
doi = {10.1145/215465.215579},
booktitle = {Proceedings of the 1995 ACM SIGPLAN Symposium on Partial Evaluation and Semantics-Based Program Manipulation},
pages = {146–155},
numpages = {10},
location = {La Jolla, California, USA},
series = {PEPM '95}
}



@inbook{taha-2004,
	abstract = {Multi-stage programming (MSP) is a paradigm for developing generic software that does not pay a runtime penalty for this generality. This is achieved through concise, carefully-designed language extensions that support runtime code generation and program execution. Additionally, type systems for MSP languages are designed to statically ensure that dynamically generated programs are type-safe, and therefore require no type checking after they are generated.},
	address = {Berlin, Heidelberg},
	author = {Taha, Walid},
	booktitle = {Domain-Specific Program Generation: International Seminar, Dagstuhl Castle, Germany, March 23-28, 2003. Revised Papers},
	doi = {10.1007/978-3-540-25935-0_3},
	editor = {Lengauer, Christian and Batory, Don and Consel, Charles and Odersky, Martin},
	isbn = {978-3-540-25935-0},
	pages = {30--50},
	publisher = {Springer Berlin Heidelberg},
	title = {A Gentle Introduction to Multi-stage Programming},
	url = {https://doi.org/10.1007/978-3-540-25935-0_3},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-540-25935-0_3}}

@article{robinson-1965,
author = {Robinson, J. A.},
title = {A Machine-Oriented Logic Based on the Resolution Principle},
year = {1965},
issue_date = {Jan. 1965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {0004-5411},
url = {https://doi.org/10.1145/321250.321253},
doi = {10.1145/321250.321253},
journal = {J. ACM},
month = {jan},
pages = {23–41},
numpages = {19}
}

@inproceedings{pottier-2014,
author = {Pottier, Fran\c{c}ois},
title = {Hindley-milner elaboration in applicative style: functional pearl},
year = {2014},
isbn = {9781450328739},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2628136.2628145},
doi = {10.1145/2628136.2628145},
abstract = {Type inference - the problem of determining whether a program is well-typed - is well-understood. In contrast, elaboration - the task of constructing an explicitly-typed representation of the program - seems to have received relatively little attention, even though, in a non-local type inference system, it is non-trivial. We show that the constraint-based presentation of Hindley-Milner type inference can be extended to deal with elaboration, while preserving its elegance. This involves introducing a new notion of "constraint with a value", which forms an applicative functor.},
booktitle = {Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming},
pages = {203–212},
numpages = {10},
keywords = {type inference, polymorphism, elaboration, constraints},
location = {Gothenburg, Sweden},
series = {ICFP '14}
}

@book{hennessy-patterson-2017,
author = {Hennessy, John L. and Patterson, David A.},
title = {Computer Architecture, Sixth Edition: A Quantitative Approach},
year = {2017},
isbn = {0128119055},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {6th},
abstract = {Computer Architecture: A Quantitative Approach, Sixth Edition has been considered essential reading by instructors, students and practitioners of computer design for over 20 years. The sixth edition of this classic textbook is fully revised with the latest developments in processor and system architecture. It now features examples from the RISC-V (RISC Five) instruction set architecture, a modern RISC instruction set developed and designed to be a free and openly adoptable standard. It also includes a new chapter on domain-specific architectures and an updated chapter on warehouse-scale computing that features the first public information on Google's newest WSC. True to its original mission of demystifying computer architecture, this edition continues the longstanding tradition of focusing on areas where the most exciting computing innovation is happening, while always keeping an emphasis on good engineering design. Includes a new chapter on domain-specific architectures, explaining how they are the only path forward for improved performance and energy efficiency given the end of Moores Law and Dennard scaling Features the first publication of several DSAs from industry Features extensive updates to the chapter on warehouse-scale computing, with the first public information on the newest Google WSC Offers updates to other chapters including new material dealing with the use of stacked DRAM; data on the performance of new NVIDIA Pascal GPU vs. new AVX-512 Intel Skylake CPU; and extensive additions to content covering multicore architecture and organization Includes "Putting It All Together" sections near the end of every chapter, providing real-world technology examples that demonstrate the principles covered in each chapter Includes review appendices in the printed text and additional reference appendices available online Includes updated and improved case studies and exercises}
}

@inproceedings{boehm-1993,
author = {Boehm, Hans-Juergen},
title = {Space efficient conservative garbage collection},
year = {1993},
isbn = {0897915984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/155090.155109},
doi = {10.1145/155090.155109},
abstract = {We call a garbage collector conservative if it has only partial information about the location of pointers, and is thus forced to treat arbitrary bit patterns as though they might be pointers, in at least some cases. We show that some very inexpensive, but previously unused techniques can have dramatic impact on the effectiveness of conservative garbage collectors in reclaiming memory. Our most significant observation is that static data that appears to point to the heap should not result in misidentified references to the heap. The garbage collector has enough information to allocate around such references. We also observe that programming style has a significant impact on the amount of spuriously retained storage, typically even if the collector is not terribly conservative. Some fairly common C and C++ programming style significantly decrease the effectiveness of any garbage collector. These observations suffice to explain some of the different assessments of conservative collection that have appeared in the literature.},
booktitle = {Proceedings of the ACM SIGPLAN 1993 Conference on Programming Language Design and Implementation},
pages = {197–206},
numpages = {10},
location = {Albuquerque, New Mexico, USA},
series = {PLDI '93}
}

@inproceedings{reinking-2021,
author = {Reinking, Alex and Xie, Ningning and de Moura, Leonardo and Leijen, Daan},
title = {Perceus: garbage free reference counting with reuse},
year = {2021},
isbn = {9781450383912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453483.3454032},
doi = {10.1145/3453483.3454032},
abstract = {We introduce Perceus, an algorithm for precise reference counting with reuse and specialization. Starting from a functional core language with explicit control-flow, Perceus emits precise reference counting instructions such that (cycle-free) programs are _garbage free_, where only live references are retained. This enables further optimizations, like reuse analysis that allows for guaranteed in-place updates at runtime. This in turn enables a novel programming paradigm that we call _functional but in-place_ (FBIP). Much like tail-call optimization enables writing loops with regular function calls, reuse analysis enables writing in-place mutating algorithms in a purely functional way. We give a novel formalization of reference counting in a linear resource calculus, and prove that Perceus is sound and garbage free. We show evidence that Perceus, as implemented in Koka, has good performance and is competitive with other state-of-the-art memory collectors.},
booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {96–111},
numpages = {16},
keywords = {Algebraic Effects, Handlers, Reference Counting},
location = {Virtual, Canada},
series = {PLDI 2021}
}

@article{mccarthy-1960,
author = {McCarthy, John},
title = {Recursive functions of symbolic expressions and their computation by machine, Part I},
year = {1960},
issue_date = {April 1960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/367177.367199},
doi = {10.1145/367177.367199},
journal = {Commun. ACM},
month = {apr},
pages = {184–195},
numpages = {12}
}

@article{bacon-2003,
author = {Bacon, David F. and Cheng, Perry and Rajan, V. T.},
title = {A real-time garbage collector with low overhead and consistent utilization},
year = {2003},
issue_date = {January 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1},
issn = {0362-1340},
url = {https://doi.org/10.1145/640128.604155},
doi = {10.1145/640128.604155},
abstract = {Now that the use of garbage collection in languages like Java is becoming widely accepted due to the safety and software engineering benefits it provides, there is significant interest in applying garbage collection to hard real-time systems. Past approaches have generally suffered from one of two major flaws: either they were not provably real-time, or they imposed large space overheads to meet the real-time bounds. We present a mostly non-moving, dynamically defragmenting collector that overcomes both of these limitations: by avoiding copying in most cases, space requirements are kept low; and by fully incrementalizing the collector we are able to meet real-time bounds. We implemented our algorithm in the Jikes RVM and show that at real-time resolution we are able to obtain mutator utilization rates of 45\% with only 1.6--2.5 times the actual space required by the application, a factor of 4 improvement in utilization over the best previously published results. Defragmentation causes no more than 4\% of the traced data to be copied.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {285–298},
numpages = {14},
keywords = {utilization, real-time scheduling, read barrier, defragmentation}
}

@inproceedings{zhao-2022,
author = {Zhao, Wenyu and Blackburn, Stephen M. and McKinley, Kathryn S.},
title = {Low-latency, high-throughput garbage collection},
year = {2022},
isbn = {9781450392655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3519939.3523440},
doi = {10.1145/3519939.3523440},
abstract = {To achieve short pauses, state-of-the-art concurrent copying collectors such as C4, Shenandoah, and ZGC use substantially more CPU cycles and memory than simpler collectors. They suffer from design limitations: i) concurrent copying with inherently expensive read and write barriers, ii) scalability limitations due to tracing, and iii) immediacy limitations for mature objects that impose memory overheads.

This paper takes a different approach to optimizing responsiveness and throughput. It uses the insight that regular, brief stop-the-world collections deliver sufficient responsiveness at greater efficiency than concurrent evacuation. It introduces LXR, where stop-the-world collections use reference counting (RC) and judicious copying. RC delivers scalability and immediacy, promptly reclaiming young and mature objects. RC, in a hierarchical Immix heap structure, reclaims most memory without any copying. Occasional concurrent tracing identifies cyclic garbage. LXR introduces: i) RC remembered sets for judicious copying of mature objects; ii) a novel low-overhead write barrier that combines coalescing reference counting, concurrent tracing, and remembered set maintenance; iii) object reclamation while performing a concurrent trace; iv) lazy processing of decrements; and v) novel survival rate triggers that modulate pause durations.

LXR combines excellent responsiveness and throughput, improving over production collectors. On the widely-used Lucene search engine in a tight heap, LXR delivers 7.8\texttimes{} better throughput and 10\texttimes{} better 99.99\% tail latency than Shenandoah. On 17 diverse modern workloads in a moderate heap, LXR outperforms OpenJDK’s default G1 on throughput by 4\% and Shenandoah by 43\%.},
booktitle = {Proceedings of the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {76–91},
numpages = {16},
keywords = {Garbage collection, Reference counting},
location = {San Diego, CA, USA},
series = {PLDI 2022}
}

@inproceedings{carpen-amarie-2023,
author = {Carpen-Amarie, Maria and Vavouliotis, Georgios and Tovletoglou, Konstantinos and Grot, Boris and Mueller, Rene},
title = {Concurrent GCs and Modern Java Workloads: A Cache Perspective},
year = {2023},
isbn = {9798400701795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3591195.3595269},
doi = {10.1145/3591195.3595269},
abstract = {The garbage collector (GC) is a crucial component of language runtimes, offering correctness guarantees and high productivity in exchange for a run-time overhead. Concurrent collectors run alongside application threads (mutators) and share CPU resources. A likely point of contention between mutators and GC threads and, consequently, a potential overhead source is the shared last-level cache (LLC). This work builds on the hypothesis that the cache pollution caused by concurrent GCs hurts application performance. We validate this hypothesis with a cache-sensitive Java micro-benchmark. We find that concurrent GC activity may slow down the application by up to 3\texttimes{} and increase the LLC misses by 3 orders of magnitude. However, when we extend our analysis to a suite of benchmarks representative for today’s server workloads (Renaissance), we find that only 5 out of 23 benchmarks show a statistically significant correlation between GC-induced cache pollution and performance. Even for these, the performance overhead of GC does not exceed 10\%. Based on further analysis, we conclude that the lower impact of the GC on the performance of Renaissance benchmarks is due to their lack of sensitivity to LLC capacity.},
booktitle = {Proceedings of the 2023 ACM SIGPLAN International Symposium on Memory Management},
pages = {71–84},
numpages = {14},
keywords = {garbage collection, cache pollution, ZGC, JVM},
location = {Orlando, FL, USA},
series = {ISMM 2023}
}

@inproceedings{foderaro-1981,
author = {Foderaro, John K. and Fateman, Richard J.},
title = {Characterization of VAX Macsyma},
year = {1981},
isbn = {0897910478},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800206.806364},
doi = {10.1145/800206.806364},
abstract = {The algebraic manipulation system Macsyma [Grou77, Fate80] has been running for over a year on Digital Equipment Corp. VAX-11 large-address-space medium-scale computers [Stre78]. In order to run Macsyma in this environment, a Lisp system for the VAX, FRANZ LISP[Fode50], was constructed at Berkeley. The goal of running Macsyma provided direction and motivation and is partially responsible for the rapid development of the Lisp system.Because Lisp is a high level language there are many decisions to be made about the internal framework of the system. Efforts to increase efficiency require that we be able to characterize the demands of a large, compiled, Lisp system. Fortunately, the VAX/UNIX operating system provides useful tools for determining such characteristics. This paper presents some of our data and related analysis.},
booktitle = {Proceedings of the Fourth ACM Symposium on Symbolic and Algebraic Computation},
pages = {14–19},
numpages = {6},
location = {Snowbird, Utah, USA},
series = {SYMSAC '81}
}

@inproceedings{wilson-1992,
author = {Wilson, Paul R.},
title = {Uniprocessor Garbage Collection Techniques},
year = {1992},
isbn = {354055940X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Proceedings of the International Workshop on Memory Management},
pages = {1–42},
numpages = {42},
series = {IWMM '92}
}

@phdthesis{zorn-1989,
author = {Zorn, Benjamin Goth},
title = {Comparative performance evaluation of garbage collection algorithms},
year = {1989},
school = {University of California, Berkeley},
publisher = {University of California, Berkeley},
abstract = {This thesis shows that object-level, trace-driven simulation can facilitate evaluation of language runtime systems and reaches new conclusions about the relative performance of important garbage collection algorithms. In particular, I reach the unexpected conclusion that mark-and-sweep garbage collection, when augmented with generations, shows comparable CPU performance and much better reference locality than the more widely used copying algorithms. In the past, evaluation of garbage collection algorithms has been limited by the high cost of implementing the algorithms. Substantially different algorithms have rarely been compared in a systematic way. With the availability of high-performance, low-cost workstations, trace-driven performance evaluation of these algorithms is now economical. This thesis describes MARS, a runtime system simulator that is driven by operations on program objects, and not memory addresses. MARS has been attached to a commercial Common Lisp system and eight large Lisp applications are used in the thesis as test programs.To illustrate the advantages of the object-level tracing technique used by MARS, this thesis compares the relative performance of stop-and-copy, incremental, and mark-and-sweep collection algorithms, all organized with multiple generations. The comparative evaluation is based on several metrics: CPU overhead, reference locality, and interactive availability. Mark-and-sweep collection shows slightly higher CPU overhead than stop-and-copy collection (5\%), but requires significantly less physical memory to achieve the same page fault rate (30-40\%). Incremental collection has very good interactive availability, but implementing the read barrier on stock hardware incurs a substantial CPU overhead (30-60\%). In the future, I will use MARS to investigate other performance aspects of sophisticated runtime systems.},
note = {AAI9029086}
}

@inproceedings{sansom-1993,
author = {Sansom, Patrick M. and Peyton Jones, Simon L.},
title = {Generational garbage collection for Haskell},
year = {1993},
isbn = {089791595X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/165180.165195},
doi = {10.1145/165180.165195},
booktitle = {Proceedings of the Conference on Functional Programming Languages and Computer Architecture},
pages = {106–116},
numpages = {11},
location = {Copenhagen, Denmark},
series = {FPCA '93}
}

@inproceedings{zhao-2022,
author = {Zhao, Wenyu and Blackburn, Stephen M. and McKinley, Kathryn S.},
title = {Low-latency, high-throughput garbage collection},
year = {2022},
isbn = {9781450392655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3519939.3523440},
doi = {10.1145/3519939.3523440},
abstract = {To achieve short pauses, state-of-the-art concurrent copying collectors such as C4, Shenandoah, and ZGC use substantially more CPU cycles and memory than simpler collectors. They suffer from design limitations: i) concurrent copying with inherently expensive read and write barriers, ii) scalability limitations due to tracing, and iii) immediacy limitations for mature objects that impose memory overheads.

This paper takes a different approach to optimizing responsiveness and throughput. It uses the insight that regular, brief stop-the-world collections deliver sufficient responsiveness at greater efficiency than concurrent evacuation. It introduces LXR, where stop-the-world collections use reference counting (RC) and judicious copying. RC delivers scalability and immediacy, promptly reclaiming young and mature objects. RC, in a hierarchical Immix heap structure, reclaims most memory without any copying. Occasional concurrent tracing identifies cyclic garbage. LXR introduces: i) RC remembered sets for judicious copying of mature objects; ii) a novel low-overhead write barrier that combines coalescing reference counting, concurrent tracing, and remembered set maintenance; iii) object reclamation while performing a concurrent trace; iv) lazy processing of decrements; and v) novel survival rate triggers that modulate pause durations.

LXR combines excellent responsiveness and throughput, improving over production collectors. On the widely-used Lucene search engine in a tight heap, LXR delivers 7.8\texttimes{} better throughput and 10\texttimes{} better 99.99\% tail latency than Shenandoah. On 17 diverse modern workloads in a moderate heap, LXR outperforms OpenJDK’s default G1 on throughput by 4\% and Shenandoah by 43\%.},
booktitle = {Proceedings of the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {76–91},
numpages = {16},
keywords = {Reference counting, Garbage collection},
location = {San Diego, CA, USA},
series = {PLDI 2022}
}


@article{pretnar-2015,
	abstract = {This paper is a tutorial on algebraic effects and handlers. In it, we explain what algebraic effects are, give ample examples to explain how handlers work, define an operational semantics and a type & effect system, show how one can reason about effects, and give pointers for further reading.},
	author = {Matija Pretnar},
	doi = {https://doi.org/10.1016/j.entcs.2015.12.003},
	issn = {1571-0661},
	journal = {Electronic Notes in Theoretical Computer Science},
	keywords = {algebraic effects, handlers, effect system, semantics, logic, tutorial},
	note = {The 31st Conference on the Mathematical Foundations of Programming Semantics (MFPS XXXI).},
	pages = {19-35},
	title = {An Introduction to Algebraic Effects and Handlers. Invited tutorial paper},
	url = {https://www.sciencedirect.com/science/article/pii/S1571066115000705},
	volume = {319},
	year = {2015},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1571066115000705},
	bdsk-url-2 = {https://doi.org/10.1016/j.entcs.2015.12.003}}

@misc{bauer-2019,
      title={What is algebraic about algebraic effects and handlers?}, 
      author={Andrej Bauer},
      year={2019},
      eprint={1807.05923},
      archivePrefix={arXiv},
      primaryClass={cs.LO},
      url={https://arxiv.org/abs/1807.05923}, 
}

@article{xie-2024,
author = {Xie, Ningning and Johnson, Daniel D. and Maclaurin, Dougal and Paszke, Adam},
title = {Parallel Algebraic Effect Handlers},
year = {2024},
issue_date = {August 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {ICFP},
url = {https://doi.org/10.1145/3674651},
doi = {10.1145/3674651},
abstract = {Algebraic effect handlers support composable and structured control-flow abstraction. However, existing designs of algebraic effects often require effects to be executed sequentially. This paper studies parallel algebraic effect handlers. In particular, we formalize λp, a lambda calculus which models two key features, effect handlers and parallelizable computations, the latter of which takes the form of a for expression, inspired by the Dex programming language. We present various interesting examples expressible in our calculus. To show that our design can be implemented in a type-safe way, we present a higher-order polymorphic lambda calculus Fp that extends λp with a lightweight value dependent type system, and prove that Fp preserves the semantics of λp and enjoys syntactic type soundness. Lastly, we provide an implementation of the language design as a Haskell library, which mirrors both λp and Fp and reveals new connections to free applicative functors. All examples presented can be encoded in the Haskell implementation. We believe this paper is the first to study the combination of user-defined effect handlers and parallel computations, and it is our hope that it provides a basis for future designs and implementations of parallel algebraic effect handlers.},
journal = {Proc. ACM Program. Lang.},
month = aug,
articleno = {262},
numpages = {33},
keywords = {Effect handlers, Parallelism, Type systems}
}


@article{kisyelov-2014,
	abstract = {We describe the first implementation of multi-prompt delimited control operators in OCaml that is direct in that it captures only the needed part of the control stack. The implementation is a library that requires no changes to the OCaml compiler or run-time, so it is perfectly compatible with existing OCaml source and binary code. The library has been in fruitful practical use since 2006. We present the library as an implementation of an abstract machine derived by elaborating the definitional machine. The abstract view lets us distill a minimalistic API, scAPI, sufficient for implementing multi-prompt delimited control. We argue that a language system that supports exception and stack-overflow handling supports scAPI. With byte- and native-code OCaml systems as two examples, our library illustrates how to use scAPI to implement multi-prompt delimited control in a typed language. The approach is general and has been used to add multi-prompt delimited control to other existing language systems.},
	author = {Oleg Kiselyov},
	doi = {https://doi.org/10.1016/j.tcs.2012.02.025},
	issn = {0304-3975},
	journal = {Theoretical Computer Science},
	keywords = {Delimited continuation, Exception, Semantics, Implementation, Abstract machine},
	note = {Functional and Logic Programming},
	pages = {56-76},
	title = {Delimited control in OCaml, abstractly and concretely},
	url = {https://www.sciencedirect.com/science/article/pii/S0304397512001661},
	volume = {435},
	year = {2012},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0304397512001661},
	bdsk-url-2 = {https://doi.org/10.1016/j.tcs.2012.02.025}}

@misc{phippscostin-2023,
      title={Continuing WebAssembly with Effect Handlers}, 
      author={Luna Phipps-Costin and Andreas Rossberg and Arjun Guha and Daan Leijen and Daniel Hillerström and KC Sivaramakrishnan and Matija Pretnar and Sam Lindley},
      year={2023},
      eprint={2308.08347},
      archivePrefix={arXiv},
      primaryClass={cs.PL},
      url={https://arxiv.org/abs/2308.08347}, 
}

@article{kiselyov-2018,
   title={Eff Directly in OCaml},
   volume={285},
   ISSN={2075-2180},
   url={http://dx.doi.org/10.4204/EPTCS.285.2},
   DOI={10.4204/eptcs.285.2},
   journal={Electronic Proceedings in Theoretical Computer Science},
   publisher={Open Publishing Association},
   author={Kiselyov, Oleg and Sivaramakrishnan, KC},
   year={2018},
   month=dec, pages={23–58} }


@article{forster-2017,
author = {Forster, Yannick and Kammar, Ohad and Lindley, Sam and Pretnar, Matija},
title = {On the expressive power of user-defined effects: effect handlers, monadic reflection, delimited control},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {ICFP},
url = {https://doi.org/10.1145/3110257},
doi = {10.1145/3110257},
abstract = {We compare the expressive power of three programming abstractions for user-defined computational effects: Plotkin and Pretnar's effect handlers, Filinski's monadic reflection, and delimited control without answer-type-modification. This comparison allows a precise discussion about the relative expressiveness of each programming abstraction. It also demonstrates the sensitivity of the relative expressiveness of user-defined effects to seemingly orthogonal language features. We present three calculi, one per abstraction, extending Levy's call-by-push-value. For each calculus, we present syntax, operational semantics, a natural type-and-effect system, and, for effect handlers and monadic reflection, a set-theoretic denotational semantics. We establish their basic metatheoretic properties: safety, termination, and, where applicable, soundness and adequacy. Using Felleisen's notion of a macro translation, we show that these abstractions can macro-express each other, and show which translations preserve typeability. We use the adequate finitary set-theoretic denotational semantics for the monadic calculus to show that effect handlers cannot be macro-expressed while preserving typeability either by monadic reflection or by delimited control. Our argument fails with simple changes to the type system such as polymorphism and inductive types. We supplement our development with a mechanised Abella formalisation.},
journal = {Proc. ACM Program. Lang.},
month = aug,
articleno = {13},
numpages = {29},
keywords = {algebraic effects and handlers, call-by-push-value, computational effects, delimited control, denotational semantics, lambda calculus, language extension, macro expressiveness, monadic reflection, monads, reify and reflect, shift and reset, type-and-effect systems}
}


@InProceedings{pirog-2019,
  author =	{Pir\'{o}g, Maciej and Polesiuk, Piotr and Sieczkowski, Filip},
  title =	{{Typed Equivalence of Effect Handlers and Delimited Control}},
  booktitle =	{4th International Conference on Formal Structures for Computation and Deduction (FSCD 2019)},
  pages =	{30:1--30:16},
  series =	{Leibniz International Proceedings in Informatics (LIPIcs)},
  ISBN =	{978-3-95977-107-8},
  ISSN =	{1868-8969},
  year =	{2019},
  volume =	{131},
  editor =	{Geuvers, Herman},
  publisher =	{Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik},
  address =	{Dagstuhl, Germany},
  URL =		{https://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.FSCD.2019.30},
  URN =		{urn:nbn:de:0030-drops-105376},
  doi =		{10.4230/LIPIcs.FSCD.2019.30},
  annote =	{Keywords: type-and-effect systems, algebraic effects, delimited control, macro expressibility}
}

@article{peyton-jones-2002,
author = {Peyton Jones, Simon and Marlow, Simon},
title = {Secrets of the Glasgow Haskell Compiler inliner},
year = {2002},
issue_date = {July 2002},
publisher = {Cambridge University Press},
address = {USA},
volume = {12},
number = {5},
issn = {0956-7968},
url = {https://doi.org/10.1017/S0956796802004331},
doi = {10.1017/S0956796802004331},
abstract = {Higher-order languages such as Haskell encourage the programmer to build abstractions by composing functions. A good compiler must inline many of these calls to recover an efficiently executable program. In principle, inlining is dead simple: just replace the call of a function by an instance of its body. But any compiler-writer will tell you that inlining is a black art, full of delicate compromises that work together to give good performance without unnecessary code bloat. The purpose of this paper is, therefore, to articulate the key lessons we learned from a full-scale “production” inliner, the one used in the Glasgow Haskell compiler. We focus mainly on the algorithmic aspects, but we also provide some indicative measurements to substantiate the importance of various aspects of the inliner.},
journal = {J. Funct. Program.},
month = jul,
pages = {393–434},
numpages = {42}
}

@inproceedings{fluet-weeks-2001,
author = {Fluet, Matthew and Weeks, Stephen},
title = {Contification using dominators},
year = {2001},
isbn = {1581134150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/507635.507639},
doi = {10.1145/507635.507639},
abstract = {Contification is a compiler optimization that turns a function that always returns to the same place into a continuation. Compilers for functional languages use contification to expose the control-flow information that is required by many optimizations, including traditional loop optimizations. This paper gives a formal presentation of contification in MLton, a whole-program optimizing Standard ML compiler. We present two existing algorithms for contification in our framework, as well as a new algorithm based on the dominator tree of a program's call graph. We prove that the dominator algorithm is optimal. We present benchmark results on realistic SML programs demonstrating that contification has minimal overhead on compile time and significantly improves run time.},
booktitle = {Proceedings of the Sixth ACM SIGPLAN International Conference on Functional Programming},
pages = {2–13},
numpages = {12},
location = {Florence, Italy},
series = {ICFP '01}
}


@article{willsey-2021,
author = {Willsey, Max and Nandi, Chandrakana and Wang, Yisu Remy and Flatt, Oliver and Tatlock, Zachary and Panchekha, Pavel},
title = {egg: Fast and extensible equality saturation},
year = {2021},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {POPL},
url = {https://doi.org/10.1145/3434304},
doi = {10.1145/3434304},
abstract = {An e-graph efficiently represents a congruence relation over many expressions. Although they were originally developed in the late 1970s for use in automated theorem provers, a more recent technique known as equality saturation repurposes e-graphs to implement state-of-the-art, rewrite-driven compiler optimizations and program synthesizers. However, e-graphs remain unspecialized for this newer use case. Equality saturation workloads exhibit distinct characteristics and often require ad-hoc e-graph extensions to incorporate transformations beyond purely syntactic rewrites. This work contributes two techniques that make e-graphs fast and extensible, specializing them to equality saturation. A new amortized invariant restoration technique called rebuilding takes advantage of equality saturation's distinct workload, providing asymptotic speedups over current techniques in practice. A general mechanism called e-class analyses integrates domain-specific analyses into the e-graph, reducing the need for ad hoc manipulation. We implemented these techniques in a new open-source library called egg. Our case studies on three previously published applications of equality saturation highlight how egg's performance and flexibility enable state-of-the-art results across diverse domains.},
journal = {Proc. ACM Program. Lang.},
month = jan,
articleno = {23},
numpages = {29},
keywords = {e-graphs, equality saturation}
}

